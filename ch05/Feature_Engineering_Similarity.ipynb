{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marzzuki/blueprints-for-text-analytics-python/blob/main/ch05/Feature_Engineering_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxdyH3xGStOs"
      },
      "source": [
        "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
        "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
        "\n",
        "**If you like the book or the code examples here, please leave a friendly comment on [Amazon.com](https://www.amazon.com/Blueprints-Text-Analytics-Using-Python/dp/149207408X)!**\n",
        "<img src=\"https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/rating.png?raw=1\" width=\"100\"/>\n",
        "\n",
        "# Chapter 5:<div class='tocSkip'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B23WorAeStOw"
      },
      "source": [
        "# Feature Engineering and Syntactic Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVdXeicnStOw"
      },
      "source": [
        "## Remark<div class='tocSkip'/>\n",
        "\n",
        "The code in this notebook differs slightly from the printed book.\n",
        "\n",
        "Several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
        "\n",
        "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGkR29RFStOw"
      },
      "source": [
        "## Setup<div class='tocSkip'/>\n",
        "\n",
        "Set directory locations. If working on Google Colab: copy files and install required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDK3J2CDStOw"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "ON_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if ON_COLAB:\n",
        "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
        "    os.system(f'wget {GIT_ROOT}/ch05/setup.py')\n",
        "\n",
        "%run -i setup.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_HeDeLStOx"
      },
      "source": [
        "## Load Python Settings<div class=\"tocSkip\"/>\n",
        "\n",
        "Common imports, defaults for formatting in Matplotlib, Pandas etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgkkTYgSStOx"
      },
      "outputs": [],
      "source": [
        "%run \"$BASE_DIR/settings.py\"\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%config InlineBackend.figure_format = 'png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "245qYMndStOx"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H5JNY4wStOx"
      },
      "outputs": [],
      "source": [
        "sentences = [\"It was the best of times\",\n",
        "             \"it was the worst of times\",\n",
        "             \"it was the age of wisdom\",\n",
        "             \"it was the age of foolishness\"]\n",
        "\n",
        "tokenized_sentences = [[t for t in sentence.split()] for sentence in sentences]\n",
        "\n",
        "vocabulary = set([w for s in tokenized_sentences for w in s])\n",
        "\n",
        "import pandas as pd\n",
        "[[w, i] for i,w in enumerate(vocabulary)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5t46tGStOy"
      },
      "source": [
        "# One-hot by hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg4dUzMtStOy"
      },
      "outputs": [],
      "source": [
        "def onehot_encode(tokenized_sentence):\n",
        "    return [1 if w in tokenized_sentence else 0 for w in vocabulary]\n",
        "\n",
        "onehot = [onehot_encode(tokenized_sentence) for tokenized_sentence in tokenized_sentences]\n",
        "\n",
        "for (sentence, oh) in zip(sentences, onehot):\n",
        "    print(\"%s: %s\" % (oh, sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYOAk2N2StOy"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(onehot, columns=list(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OavBlzl3StOy"
      },
      "outputs": [],
      "source": [
        "sim = [onehot[0][i] & onehot[1][i] for i in range(0, len(vocabulary))]\n",
        "sum(sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWxZu-UDStOy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.dot(onehot[0], onehot[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssbGtTt6StOy"
      },
      "outputs": [],
      "source": [
        "np.dot(onehot, onehot[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4onhtEyStOy"
      },
      "source": [
        "## Out of vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyaHXxsOStOy"
      },
      "outputs": [],
      "source": [
        "onehot_encode(\"the age of wisdom is the best of times\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC_iQoAqStOy"
      },
      "outputs": [],
      "source": [
        "onehot_encode(\"John likes to watch movies. Mary likes movies too.\".split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yPiXGHYStOy"
      },
      "source": [
        "## document term matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9HCl5eBStOz"
      },
      "outputs": [],
      "source": [
        "onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5TWSDjaStOz"
      },
      "source": [
        "## similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SzzPl9rStOz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.dot(onehot, np.transpose(onehot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7QbYdV8StOz"
      },
      "source": [
        "# scikit learn one-hot vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahOa051VStOz"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "lb = MultiLabelBinarizer()\n",
        "lb.fit([vocabulary])\n",
        "lb.transform(tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptcyDaNkStOz"
      },
      "source": [
        "# CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BBU1kWfStOz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0omwvSwStOz"
      },
      "outputs": [],
      "source": [
        "more_sentences = sentences + [\"John likes to watch movies. Mary likes movies too.\",\n",
        "                              \"Mary also likes to watch football games.\"]\n",
        "pd.DataFrame(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LKTHigmStOz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGczVhVUStOz"
      },
      "outputs": [],
      "source": [
        "cv.fit(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDck0uTLStOz"
      },
      "outputs": [],
      "source": [
        "print(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYsmg0xaStOz"
      },
      "outputs": [],
      "source": [
        "dt = cv.transform(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoEHY8s7StO0"
      },
      "outputs": [],
      "source": [
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK0Re6DQStO0"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh3m0WTqStO0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(dt[0], dt[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h-u_WNDStO0"
      },
      "outputs": [],
      "source": [
        "len(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb_PCvn2StO0"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(cosine_similarity(dt, dt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-OKn2tbStO0"
      },
      "source": [
        "# TF/IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_UDpVyWStO0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer()\n",
        "tfidf_dt = tfidf.fit_transform(dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEWr1ekAStO0"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44xfsWJyStO1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(cosine_similarity(tfidf_dt, tfidf_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh8AqS_cStO1"
      },
      "outputs": [],
      "source": [
        "headlines = pd.read_csv(ABCNEWS_FILE, parse_dates=[\"publish_date\"])\n",
        "headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaPOYzbGStO1"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkpAx0qmStO1"
      },
      "outputs": [],
      "source": [
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-5xGVMkStO1"
      },
      "outputs": [],
      "source": [
        "dt.data.nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abXgQ7pbStO1"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "cosine_similarity(dt[0:10000], dt[0:10000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P419TApkStO1"
      },
      "source": [
        "## Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhLyx5RRStO1"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "print(len(stopwords))\n",
        "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s869XDMyStO2"
      },
      "source": [
        "## min_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x4t3oM2StO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jMtzsaiStO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=.0001)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfMaonjMStO2"
      },
      "source": [
        "## max_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsQiW8ENStO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stopwords, max_df=0.1)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIHwhKI_StO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_df=0.1)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScNboHm5StO2"
      },
      "source": [
        "## n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flmOyJrsStO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "print(dt.shape)\n",
        "print(dt.data.nbytes)\n",
        "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "print(dt.shape)\n",
        "print(dt.data.nbytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBpVv_bStO2"
      },
      "source": [
        "## Lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dQYnw8NStO2"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nouns_adjectives_verbs = [\"NOUN\", \"PROPN\", \"ADJ\", \"ADV\", \"VERB\"]\n",
        "for i, row in tqdm(headlines.iterrows(), total=len(headlines)):\n",
        "    doc = nlp(str(row[\"headline_text\"]))\n",
        "    headlines.at[i, \"lemmas\"] = \" \".join([token.lemma_ for token in doc])\n",
        "    headlines.at[i, \"nav\"] = \" \".join([token.lemma_ for token in doc if token.pos_ in nouns_adjectives_verbs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsZ0zmTdStO2"
      },
      "outputs": [],
      "source": [
        "headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxGjNLtGStO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
        "dt = tfidf.fit_transform(headlines[\"lemmas\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OKaXs7qStO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e68F1xb9StO2"
      },
      "source": [
        "## remove top 10,000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgJbuz9dStO2"
      },
      "outputs": [],
      "source": [
        "top_10000 = pd.read_csv(\"https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english.txt\", header=None)\n",
        "tfidf = TfidfVectorizer(stop_words=set(top_10000.iloc[:,0].values))\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ1Uvp57StO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=set(top_10000.iloc[:,0].values), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTGkWXUTStO2"
      },
      "source": [
        "## Finding document most similar to made-up document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LizGXTeeStO2"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"lemmas\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU1Wn1elStO3"
      },
      "outputs": [],
      "source": [
        "made_up = tfidf.transform([\"australia and new zealand discuss optimal apple size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-DTMFoYStO3"
      },
      "outputs": [],
      "source": [
        "sim = cosine_similarity(made_up, dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhXBTZkpStO3"
      },
      "outputs": [],
      "source": [
        "sim[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9OG61NHStO3"
      },
      "outputs": [],
      "source": [
        "headlines.iloc[np.argsort(sim[0])[::-1][0:5]][[\"publish_date\", \"lemmas\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pq9zrnlStO3"
      },
      "source": [
        "# Finding the most similar documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zf8SPv1StO3"
      },
      "outputs": [],
      "source": [
        "# there are \"test\" headlines in the corpus\n",
        "stopwords.add(\"test\")\n",
        "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2), min_df=2, norm='l2')\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRhhSMP8StO3"
      },
      "source": [
        "### Timing Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3dxES7ZStO3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "cosine_similarity(dt[0:10000], dt[0:10000], dense_output=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5vIAED4StO3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "r = cosine_similarity(dt[0:10000], dt[0:10000])\n",
        "r[r > 0.9999] = 0\n",
        "print(np.argmax(r))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOgiwjArStO3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "r = cosine_similarity(dt[0:10000], dt[0:10000], dense_output=False)\n",
        "r[r > 0.9999] = 0\n",
        "print(np.argmax(r))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbJpwBd6StO3"
      },
      "source": [
        "### Timing Dot-Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aTa_sD6StO3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "r = np.dot(dt[0:10000], np.transpose(dt[0:10000]))\n",
        "r[r > 0.9999] = 0\n",
        "print(np.argmax(r))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXtMJJE9StO3"
      },
      "source": [
        "## Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "G0Esr-M0StO3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "batch = 10000\n",
        "max_sim = 0.0\n",
        "max_a = None\n",
        "max_b = None\n",
        "for a in range(0, dt.shape[0], batch):\n",
        "    for b in range(0, a+batch, batch):\n",
        "        print(a, b)\n",
        "        #r = np.dot(dt[a:a+batch], np.transpose(dt[b:b+batch]))\n",
        "        r = cosine_similarity(dt[a:a+batch], dt[b:b+batch], dense_output=False)\n",
        "        # eliminate identical vectors\n",
        "        # by setting their similarity to np.nan which gets sorted out\n",
        "        r[r > 0.9999] = 0\n",
        "        sim = r.max()\n",
        "        if sim > max_sim:\n",
        "            # argmax returns a single value which we have to\n",
        "            # map to the two dimensions\n",
        "            (max_a, max_b) = np.unravel_index(np.argmax(r), r.shape)\n",
        "            # adjust offsets in corpus (this is a submatrix)\n",
        "            max_a += a\n",
        "            max_b += b\n",
        "            max_sim = sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr24SZrpStO3"
      },
      "outputs": [],
      "source": [
        "print(max_a, max_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMuNZa7SStO3"
      },
      "outputs": [],
      "source": [
        "print(max_sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8uBZBIcStO3"
      },
      "outputs": [],
      "source": [
        "pd.set_option('max_colwidth', -1)\n",
        "headlines.iloc[[max_a, max_b]][[\"publish_date\", \"headline_text\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbKvv5FSStO3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATOueAngStO3"
      },
      "source": [
        "# Finding most related words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9PeWe_iStO3"
      },
      "outputs": [],
      "source": [
        "tfidf_word = TfidfVectorizer(stop_words=stopwords, min_df=1000)\n",
        "dt_word = tfidf_word.fit_transform(headlines[\"headline_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOJCYpo2StO3"
      },
      "outputs": [],
      "source": [
        "r = cosine_similarity(dt_word.T, dt_word.T)\n",
        "np.fill_diagonal(r, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MXR1abtStO3"
      },
      "outputs": [],
      "source": [
        "voc = tfidf_word.get_feature_names_out()\n",
        "size = r.shape[0] # quadratic\n",
        "for index in np.argsort(r.flatten())[::-1][0:40]:\n",
        "    a = int(index/size)\n",
        "    b = index%size\n",
        "    if a > b:  # avoid repetitions\n",
        "        print('\"%s\" related to \"%s\"' % (voc[a], voc[b]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPdnYJaPStO3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}